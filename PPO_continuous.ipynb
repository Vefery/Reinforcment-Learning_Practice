{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.normal import Normal\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device(\"cuda\"))\n",
    "layer_dim = 512\n",
    "lr = 3e-4\n",
    "batch_size = 7\n",
    "total_plays = 400\n",
    "num_epochs = 5\n",
    "clip_epsilon = 0.2\n",
    "gamma = 0.97\n",
    "lmbda = 0.93\n",
    "entropy_coef = 1e-4\n",
    "grad_max = 1\n",
    "N = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "obs_dim = env.observation_space.shape[-1]\n",
    "action_dim = env.action_space.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states, dtype=torch.float), np.array(self.actions, dtype=torch.float), np.array(self.probs, dtype=torch.float), np.array(self.vals, dtype=torch.float), np.array(self.rewards, dtype=torch.float), np.array(self.dones, dtype=torch.bool), batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action.cpu())\n",
    "        self.probs.append(probs.detach().cpu())\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nets and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, layer_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.loc = nn.Linear(layer_dim, action_dim)\n",
    "        self.scale = nn.Linear(layer_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        tanh = self.actor(state)\n",
    "        loc = self.loc(tanh)\n",
    "        scale_log = self.scale(tanh)\n",
    "        scale_log = torch.clamp(scale_log, -20, 2)\n",
    "\n",
    "        return loc, scale_log\n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, 1),\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self) -> None:\n",
    "        self.actor = ActorNetwork()\n",
    "        self.critic = CriticNetwork()\n",
    "        self.memory = Memory(batch_size)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(device)\n",
    "\n",
    "        loc, scale_log = self.actor(state)\n",
    "        scale = scale_log.exp()\n",
    "        dist = Normal(loc, scale)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        probs = dist.log_prob(action).sum(1, keepdim=True)\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def train_iteration(self):\n",
    "        losses = []\n",
    "        for _ in range(num_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + gamma*values[k+1]*(1-int(dones_arr[k])) - values[k])\n",
    "                    if dones_arr[k] == 1:\n",
    "                        discount = 1\n",
    "                    else:\n",
    "                        discount *= gamma*lmbda\n",
    "                advantage[t] = a_t\n",
    "            advantage = torch.tensor(advantage).to(device)\n",
    "\n",
    "            values = torch.tensor(values).to(device)\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch]).to(device)\n",
    "                actions = torch.tensor(action_arr[batch]).to(device)\n",
    "\n",
    "                loc, scale_log = self.actor(states)\n",
    "                scale = scale_log.exp()\n",
    "                dist = Normal(loc, scale)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "\n",
    "                new_probs = torch.squeeze(dist.log_prob(actions))\n",
    "                prob_ratio = (new_probs.exp() / old_probs.exp()).mean(dim=1)\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-clip_epsilon, 1+clip_epsilon)*advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                entropy_bonus = -(new_probs.exp() * new_probs).mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss + entropy_coef * entropy_bonus\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                losses.append(total_loss)\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), grad_max)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), grad_max)\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_scripted = torch.jit.script(self.actor)\n",
    "        model_scripted.save(\"models/lander_continuous\" + \"_final.pth\")\n",
    "\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [02:10<?, ?it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]d:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\gymnasium\\envs\\classic_control\\continuous_mountain_car.py:179: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.state = np.array([position, velocity], dtype=np.float32)\n",
      "d:\\Programs\\Anaconda\\envs\\mlagents\\lib\\site-packages\\gymnasium\\envs\\classic_control\\continuous_mountain_car.py:179: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  self.state = np.array([position, velocity], dtype=np.float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'torch.float32' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation_new\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (global_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m         learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     49\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Cell \u001b[1;32mIn[35], line 70\u001b[0m, in \u001b[0;36mAgent.train_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     68\u001b[0m     state_arr, action_arr, old_prob_arr, vals_arr,\\\n\u001b[0;32m     69\u001b[0m     reward_arr, dones_arr, batches \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 70\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     values \u001b[38;5;241m=\u001b[39m vals_arr\n\u001b[0;32m     73\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(reward_arr), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[34], line 19\u001b[0m, in \u001b[0;36mMemory.generate_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(indices)\n\u001b[0;32m     17\u001b[0m batches \u001b[38;5;241m=\u001b[39m [indices[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_start]\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvals, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool), batches\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret 'torch.float32' as a data type"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=total_plays)\n",
    "pbar.reset()\n",
    "writer = SummaryWriter(\"logs/run_lander_cont\" + str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute))\n",
    "\n",
    "writer.add_text(\n",
    "         \"Hyperparameters\",\n",
    "         \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join(\n",
    "              [f\"|lr|{lr}|\",\n",
    "               f\"|Layer dim|{layer_dim}|\",\n",
    "               f\"|Frames per batch|{batch_size}|\",\n",
    "               f\"|Epochs|{num_epochs}|\",\n",
    "               f\"|Gamma|{gamma}|\",\n",
    "               f\"|Lambda|{lmbda}|\",\n",
    "               f\"|Clip eps|{clip_epsilon}|\",\n",
    "               f\"|Steps per decend|{N}|\",\n",
    "               f\"|Entropy coef|{entropy_coef}|\",\n",
    "               ]\n",
    "         )),\n",
    "         int(str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute)))\n",
    "\n",
    "agent = Agent()\n",
    "#agent.actor.apply(weights_init_uniform_rule)\n",
    "#agent.critic.apply(weights_init_uniform_rule)\n",
    "score_history = []\n",
    "best_score = -1000000\n",
    "learn_iters = 0\n",
    "global_steps = 0\n",
    "avg_score = 0\n",
    "for i in range(total_plays):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    iter_steps = 0\n",
    "    losses = []\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_new, reward, terminated, truncated, _ = env.step(action.cpu())\n",
    "        done = terminated or truncated\n",
    "        score += reward\n",
    "        global_steps += 1\n",
    "        iter_steps += 1\n",
    "        agent.memory.store_memory(observation, action, prob, val, reward, done)\n",
    "        observation = observation_new\n",
    "        \n",
    "        if (global_steps % N == 0):\n",
    "            losses = agent.train_iteration()\n",
    "            learn_iters += 1\n",
    "\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if (best_score < avg_score):\n",
    "        best_score = avg_score\n",
    "        agent.save_model()\n",
    "    \n",
    "    writer.add_scalar(\"charts/reward\", avg_score, global_step=i)\n",
    "    writer.add_scalar(\"charts/step_count\", iter_steps, global_step=i)\n",
    "    pbar.update()\n",
    "\n",
    "print(learn_iters)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'actor_state_dict': agent.actor.state_dict(),\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': agent.critic_optimizer.state_dict(),\n",
    "            'critic_state_dict': agent.critic.state_dict(),\n",
    "            }, \"models/pole\" + str(i) + \"_steps_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"models/pole999_steps_weights.pt\")\n",
    "agent.actor.load_state_dict(ckpt['actor_state_dict'])\n",
    "agent.actor_optimizer.load_state_dict(ckpt['actor_optimizer_state_dict'])\n",
    "agent.critic_optimizer.load_state_dict(ckpt['critic_optimizer_state_dict'])\n",
    "agent.critic.load_state_dict(ckpt['critic_state_dict'])\n",
    "agent.actor.train()\n",
    "agent.critic.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
