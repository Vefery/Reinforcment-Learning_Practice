{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device(\"cuda\"))\n",
    "layer_dim = 512\n",
    "lr = 3e-4\n",
    "batch_size = 64\n",
    "total_plays = 10000\n",
    "num_epochs = 10\n",
    "clip_epsilon = 0.2\n",
    "gamma = 0.97\n",
    "lmbda = 0.93\n",
    "entropy_coef = 1e-4\n",
    "grad_max = 1\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"LunarLander-v2\", wind_power=15, turbulence_power=1.5)\n",
    "\n",
    "unity_env = UnityEnvironment(\"D:\\Practice\\SentisInfrence\\Build\\SentisInfrence.exe\", no_graphics=True)\n",
    "env = UnityToGymWrapper(unity_env)\n",
    "\n",
    "obs_dim = env.observation_space.shape[-1]\n",
    "action_dim = int(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nets and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.LinearLR(self.optimizer, start_factor=1, end_factor=0, total_iters=total_plays / 2)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "\n",
    "        return dist\n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, 1),\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.LinearLR(self.optimizer, start_factor=1, end_factor=0, total_iters=total_plays / 2)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self) -> None:\n",
    "        self.actor = ActorNetwork()\n",
    "        self.critic = CriticNetwork()\n",
    "        self.memory = Memory(batch_size)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def train_iteration(self):\n",
    "        losses = []\n",
    "        for _ in range(num_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + gamma*values[k+1]*(1-int(dones_arr[k])) - values[k])\n",
    "                    if dones_arr[k] == 1:\n",
    "                        discount = 1\n",
    "                    else:\n",
    "                        discount *= gamma*lmbda\n",
    "                advantage[t] = a_t\n",
    "            advantage = torch.tensor(advantage).to(device)\n",
    "\n",
    "            values = torch.tensor(values).to(device)\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch]).to(device)\n",
    "                actions = torch.tensor(action_arr[batch]).to(device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                dist = Categorical(dist)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-clip_epsilon, 1+clip_epsilon)*advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                entropy_bonus = -(new_probs.exp() * new_probs).mean()\n",
    "\n",
    "                total_loss = actor_loss - 0.5*critic_loss + entropy_coef * entropy_bonus\n",
    "\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                losses.append(total_loss)\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), grad_max)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), grad_max)\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_scripted = torch.jit.script(self.actor)\n",
    "        model_scripted.save(\"models/lander\" + \"_final.pth\")\n",
    "\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "agent = Agent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pbar\" in globals():\n",
    "    pbar.close()\n",
    "pbar = tqdm(total=total_plays)\n",
    "pbar.reset()\n",
    "writer = SummaryWriter(\"logs/unity\" + str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute))\n",
    "\n",
    "writer.add_text(\n",
    "         \"Hyperparameters\",\n",
    "         \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join(\n",
    "              [f\"|lr|{lr}|\",\n",
    "               f\"|Layer dim|{layer_dim}|\",\n",
    "               f\"|Frames per batch|{batch_size}|\",\n",
    "               f\"|Epochs|{num_epochs}|\",\n",
    "               f\"|Gamma|{gamma}|\",\n",
    "               f\"|Lambda|{lmbda}|\",\n",
    "               f\"|Clip eps|{clip_epsilon}|\",\n",
    "               f\"|Steps per decend|{N}|\",\n",
    "               f\"|Entropy coef|{entropy_coef}|\",\n",
    "               ]\n",
    "         )),\n",
    "         int(str(datetime.now().day) + str(datetime.now().hour) + str(datetime.now().minute)))\n",
    "\n",
    "#agent.actor.apply(weights_init_uniform_rule)\n",
    "#agent.critic.apply(weights_init_uniform_rule)\n",
    "score_history = []\n",
    "best_score = env.reward_range[0]\n",
    "learn_iters = 0\n",
    "global_steps = 0\n",
    "avg_score = 0\n",
    "for i in range(total_plays):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    iter_steps = 0\n",
    "    losses = []\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_new, reward, terminated, _ = env.step(action)\n",
    "        done = terminated\n",
    "        score += reward\n",
    "        global_steps += 1\n",
    "        iter_steps += 1\n",
    "        agent.memory.store_memory(observation, action, prob, val, reward, terminated)\n",
    "        observation = observation_new\n",
    "        \n",
    "        if (global_steps % N == 0):\n",
    "            losses = agent.train_iteration()\n",
    "            learn_iters += 1\n",
    "            writer.add_scalar(\"charts/loss\", sum(losses) / len(losses), global_step=i)\n",
    "\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if global_steps > total_plays / 2:\n",
    "        agent.critic.scheduler.step()\n",
    "        agent.actor.scheduler.step()\n",
    "    \n",
    "    writer.add_scalar(\"charts/reward\", avg_score, global_step=i)\n",
    "    writer.add_scalar(\"charts/step_count\", iter_steps, global_step=i)\n",
    "    pbar.update()\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'actor_state_dict': agent.actor.state_dict(),\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': agent.critic_optimizer.state_dict(),\n",
    "            'critic_state_dict': agent.critic.state_dict(),\n",
    "            }, \"models/lander\" + str(i) + \"_steps_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"models/lander399_steps_weights.pt\")\n",
    "agent.actor.load_state_dict(ckpt['actor_state_dict'])\n",
    "agent.actor_optimizer.load_state_dict(ckpt['actor_optimizer_state_dict'])\n",
    "agent.critic_optimizer.load_state_dict(ckpt['critic_optimizer_state_dict'])\n",
    "agent.critic.load_state_dict(ckpt['critic_state_dict'])\n",
    "agent.actor.train()\n",
    "agent.critic.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
